{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b399f8-a786-47c1-958f-7f648cc3cca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[32m     27\u001b[39m RANDOM_STATE = \u001b[32m42\u001b[39m\n\u001b[32m     28\u001b[39m np.random.seed(RANDOM_STATE)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import (accuracy_score\n",
    ",classification_report\n",
    ",confusion_matrix\n",
    ",roc_auc_score\n",
    ",balanced_accuracy_score\n",
    ",roc_curve\n",
    ",auc\n",
    ",precision_score\n",
    ",recall_score\n",
    ",f1_score) \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24288a3a-5023-42cf-adaa-ab1d05dfc1f9",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e9110-9fa7-4448-8c82-986c4f50479e",
   "metadata": {},
   "source": [
    "**We download daily S&P 500 price data from 2010 through today using Yahoo Finance. This dataset includes open, high, low, close, adjusted prices, and volume. Data is sorted chronologically to maintain proper time-series structure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb501ad-84ea-403b-ace1-e27b708f3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "start_date = \"2010-01-01\"\n",
    "end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "data = yf.download(\"^GSPC\", start=start_date, end=\"2025-12-07\", auto_adjust=True) #Adjusted so stakeholder can see original results - AA\n",
    "data = data.reset_index()\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "# Fix MultiIndex issues\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    data.columns = data.columns.get_level_values(0)\n",
    "\n",
    "print(\"Data rows:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796891e-dd1d-4929-836b-d56693a613d5",
   "metadata": {},
   "source": [
    "## Create Target Up/Down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577de46e-490c-44e4-a324-7393214c7b40",
   "metadata": {},
   "source": [
    "**We defined the target direction as:**\n",
    "\n",
    "- 1 = Next day's close is higher (UP)\n",
    "\n",
    "- 0 = Next day's close is lower (DOWN)\n",
    "\n",
    "The class distribution shows more UP days, which aligns with long-term market drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220dd2d2-1b72-4279-b499-c9e6532058d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "data['Direction'] = np.where(data['Close'].shift(-1) > data['Close'], 1, 0)\n",
    "print(data['Direction'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe0362b-cd14-4b6b-af43-2fd12550281d",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "**We generate common technical indicators:**\n",
    "\n",
    "- 1-day return + lagged returns\n",
    "- Rolling volatility\n",
    "- Simple moving averages\n",
    "- Volume lag\n",
    "- RSI\n",
    "- MACD + MACD signal\n",
    "  \n",
    "These indicators capture momentum, trend, and volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b698fd6-37f2-4888-9809-2521b8388e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "data['Return_1'] = data['Close'].pct_change(1)\n",
    "\n",
    "lags = [1,2,3,5]\n",
    "for l in lags:\n",
    "    data[f'Return_lag_{l}'] = data['Return_1'].shift(l)\n",
    "\n",
    "data['roll_std_5'] = data['Return_1'].rolling(5).std()\n",
    "data['roll_std_10'] = data['Return_1'].rolling(10).std()\n",
    "\n",
    "data['SMA_5'] = data['Close'].rolling(5).mean()\n",
    "data['SMA_10'] = data['Close'].rolling(10).mean()\n",
    "data['SMA_diff_5_10'] = data['SMA_5'] - data['SMA_10']\n",
    "\n",
    "data['Volume_lag_1'] = data['Volume'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac319687-ad93-45c5-96ac-4b278394372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AA - Extra technical Indicators that may improve model performance\n",
    "\n",
    "#we can capture return variation over previous 20 days\n",
    "# 20 days is about a month of market activity\n",
    "\n",
    "data['rolling_std_20'] = data['Return_1'].rolling(20).std()\n",
    "\n",
    "#inclusion of Bollinger bands to measure when a price unusally high or low \n",
    "\n",
    "band_window = 20\n",
    "data['SMA_20'] = data['Close'].rolling(band_window).mean()\n",
    "\n",
    "#create upper and lower boilinger bands\n",
    "\n",
    "#upper\n",
    "data['Boilinger_Upper_20'] = data['SMA_20'] + 2 * data['rolling_std_20']\n",
    "\n",
    "#lower\n",
    "data['Boilinger_Lower_20'] = data['SMA_20'] - 2 * data['rolling_std_20']\n",
    "\n",
    "#show the changes to show the short term trend direction\n",
    "data['Slope_of_SMA_20'] = data['SMA_20'].diff()\n",
    "\n",
    "#track whether the volume is going more up or down\n",
    "\n",
    "data['OBV'] =  (np.sign(data['Return_1']) * data['Volume']).cumsum()\n",
    "\n",
    "#adding price changes over the last 5 days \n",
    "\n",
    "data['momentum_5'] = data['Close'] / data['Close'].shift(5) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a8422a-8b04-43fc-8e70-4fea07dfd9d1",
   "metadata": {},
   "source": [
    "## Relative Strength Index (RSI) and Moving Average Convergence Divergence (MACD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8902a-10df-4505-8739-7cc4407c76d8",
   "metadata": {},
   "source": [
    "These momentum indicators help detect overbought/oversold conditions and trend strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fa68f-8a1b-4005-9d7c-c09a2afdbcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "# RSI\n",
    "window = 14\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window).mean()\n",
    "avg_loss = loss.rolling(window).mean()\n",
    "rs = avg_gain / (avg_loss + 1e-9)\n",
    "data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# MACD\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "data['MACD_signal'] = data['MACD'].ewm(span=9, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c2dcf-719c-485c-b415-6be47dbaf7a1",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025aae2-6a30-479e-911f-dd1492650537",
   "metadata": {},
   "source": [
    "Lagged and rolling features introduced missing values in the early rows.\n",
    "    \n",
    "We removed these rows and fixed remaining NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457133f5-e930-45f6-8a9b-0abb52ba977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "max_lag = max(max(lags), 10)\n",
    "\n",
    "data = data.iloc[max_lag:].reset_index(drop=True)\n",
    "data = data[~data['Direction'].isna()].reset_index(drop=True)\n",
    "\n",
    "data['RSI'] = data['RSI'].fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af5afb-c3b1-47a6-af51-bc852167ef23",
   "metadata": {},
   "source": [
    "## Prepare x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437a69d-c3b3-4bff-8c76-3bb53ade3ae6",
   "metadata": {},
   "source": [
    "Raw price columns are removed to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4270e-ba27-448c-8355-fda66a0c58dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "drop_cols = ['Date', 'Open', 'High', 'Low', 'Close']\n",
    "X = data.drop(columns=drop_cols + ['Direction'], errors='ignore')\n",
    "y = data['Direction']\n",
    "\n",
    "print(\"Features:\", X.columns.tolist())\n",
    "print(\"Shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3235faf9-0afa-4f3a-aad6-00e3dbd727e0",
   "metadata": {},
   "source": [
    "## Train/Test Split (chronological)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a2b1a-e6d2-408a-9d38-110b0b944fec",
   "metadata": {},
   "source": [
    "We use an 80/20 chronological split:\n",
    "- **Training:** 2010–2022\n",
    "- **Testing:** 2022–2025 This simulates real forecasting conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c3709-3c1e-40fb-aba6-11066fd478cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "split_index = int(0.8 * len(X))\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "print(\"Train:\", len(X_train), \"Test:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adcf81e-c92f-40c0-8196-7cf96727d129",
   "metadata": {},
   "source": [
    "## Evaluation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45c7f8-2928-4340-9dbc-5dd96da8cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "\n",
    "#AA - updating the model evaluation with ROC_AUC\n",
    "\n",
    "#AA - ROC-AUC is better for imbalanced data so adding here. Accuracy can mislead by predicting majority class whereas ROC-AUC \n",
    "#looks at how well the model separates classes by checking true positive and true negative rates across thresholds.\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    #AA adding predicted probabilities for ROC-AUC\n",
    "\n",
    "    probs = model.predict_proba(X_test)[:, 1] #this will give probability of UP days\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "\n",
    "    #AA adding ROC-AUC\n",
    "    roc_auc = roc_auc_score(y_test, probs)\n",
    "\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
    "\n",
    "    #AA  adding the print statement for ROC_AUC\n",
    "    print(f\"{name} ROC-AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    print(classification_report(y_test, preds))\n",
    "\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "    plt.title(name + \" Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return {'accuracy':acc, 'roc_auc': roc_auc} #AA - updated print statement for ROC_AUC "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb32d5b-e2a8-4e97-af95-22d56a308049",
   "metadata": {},
   "source": [
    "## Logistic Regression (Tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede8c2e-9ad4-4532-a26b-a9881b7961f7",
   "metadata": {},
   "source": [
    "**Logistic Regression Results**\n",
    "\n",
    "Logistic Regression achieved 43.6% accuracy.\n",
    "It predicts DOWN days better than UP days but has weak performance.\n",
    "\n",
    "This is below the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c7ff9-c518-4d42-bed0-d54885aca4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=2000, random_state=RANDOM_STATE,\n",
    "                              class_weight = 'balanced'))]) # AA added to give more attention to DOWN days \n",
    "#to help predict them better and helps to avoid majority class domination.\n",
    "\n",
    "param_lr = {'clf__C': [0.01, 0.1, 1, 5]}\n",
    "\n",
    "rs_lr = RandomizedSearchCV(pipe_lr, param_lr, n_iter=4,\n",
    "                           cv=tscv, scoring='roc_auc', # AA - changed from accuracy to ROC_AUC to focus on how well \n",
    "                           #the model separates UP vs. DOWN days\n",
    "                           n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "rs_lr.fit(X_train, y_train)\n",
    "best_lr = rs_lr.best_estimator_\n",
    "\n",
    "metrics_lr = evaluate_model(\"Logistic Regression\", best_lr, X_test, y_test) #RS/AA updated naming convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff0905-6b7d-40d9-8162-723dd680a07c",
   "metadata": {},
   "source": [
    "## Random Forest (Tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f70204-ace9-4afd-a80d-6f631cad439a",
   "metadata": {},
   "source": [
    "**Random Forest Results**\n",
    "\n",
    "Random Forest achieved 44.8% accuracy, lower than the baseline.\n",
    "It performs better for DOWN days than UP days but still struggles overall.\n",
    "\n",
    "It has the highest ROC-AUC of 0.48 but is still close to random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6df897-16fc-4381-8597-f8bc7ef521a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "pipe_rf = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE\n",
    "                                  ,class_weight='balanced_subsample'))]) #AA - Added to re-weigh tree classes to help the minority class\n",
    "\n",
    "param_rf = {\n",
    "    'clf__n_estimators': [200, 300, 500],\n",
    "    'clf__max_depth': [5, 10],\n",
    "    'clf__min_samples_split': [2,5],\n",
    "    'clf__min_samples_leaf': [1,4]}\n",
    "\n",
    "rs_rf = RandomizedSearchCV(pipe_rf, param_rf, n_iter=6,\n",
    "                           cv=tscv, scoring='roc_auc', #AA -  changed from accuracy to ROC_AUC\n",
    "                           n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "rs_rf.fit(X_train, y_train)\n",
    "best_rf = rs_rf.best_estimator_\n",
    "\n",
    "metrics_rf = evaluate_model(\"Random Forest (balanced)\", best_rf, X_test, y_test) #RS/AA updated naming convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b46f18-e364-4efe-a0b8-3e2133fc92ca",
   "metadata": {},
   "source": [
    "## XGBoost (Tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40596145-b32c-4cd5-beba-287bd9390fd5",
   "metadata": {},
   "source": [
    "**XGBoost Results**\n",
    "\n",
    "XGBoost achieved 46.8% accuracy.\n",
    "It struggles to separate the overall classes but predicts DOWN days slighly better than UP days.\n",
    "\n",
    "This is below baseline and has an ROC-AUC of 0.48 which shows its close to random guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d845ef-0cf1-4440-9630-452ff9abf176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AA - need to add class balance for XGBoost\n",
    "\n",
    "#AA compute DOWN days compared to UP\n",
    "down_up_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "#AA - print out results\n",
    "print(\"Class ratio (DOWN / UP):\", down_up_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec0f286-a239-45ad-83bf-36558439d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "pipe_xgb = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', XGBClassifier(eval_metric='logloss',\n",
    "                          random_state=RANDOM_STATE,\n",
    "                          n_jobs=-1\n",
    "                         ,scale_pos_weight = down_up_ratio))]) # AA - giving more weight to DOWN by shrinking the weight of UP\n",
    "\n",
    "param_xgb = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [2, 3, 5],\n",
    "    'clf__learning_rate': [0.01, 0.1]}\n",
    "\n",
    "rs_xgb = RandomizedSearchCV(pipe_xgb, param_xgb, n_iter=6,\n",
    "                            cv=tscv, scoring='roc_auc', # AA - changed from accuracy to ROC_AUC\n",
    "                            n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "rs_xgb.fit(X_train, y_train)\n",
    "best_xgb = rs_xgb.best_estimator_\n",
    "\n",
    "# AA - updating with the updated model evaluation which includes accuracy, ROC_AUC, and confusion matrix\n",
    "metrics_xgb = evaluate_model(\"XGBoost\", best_xgb, X_test, y_test)\n",
    "print(\"XGBoost metrics:\", metrics_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57cf60f-1ee2-41b8-abbe-e2dfc18c8357",
   "metadata": {},
   "source": [
    "## LightGBM (Tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d7d18-1cd7-4daf-b136-4fd81180d4e9",
   "metadata": {},
   "source": [
    "**LightGBM Results**\n",
    "\n",
    "LightGBM achieved 47.3% accuracy.\n",
    "compared to other models, it has a more balanced performance for UP and DOWN days, but overall its still limited.\n",
    "\n",
    "It has a ROC-AUC score of 0.48 which is still close to but below random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e5b86-7e31-4cb1-88ad-ff307a43eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the addition of LightGBM to see if we get better performance - AA\n",
    "\n",
    "# Tuned LightGBM - AA\n",
    "\n",
    "pipe_lgb = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LGBMClassifier(objective='binary',\n",
    "                          random_state=RANDOM_STATE,\n",
    "                          n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipe_lgb.set_output(transform=\"pandas\") #added to clear feature name warning - AA\n",
    "\n",
    "param_lgb = {\n",
    "    'clf__n_estimators': [100, 200, 400],\n",
    "    'clf__max_depth': [-1, 3, 5],\n",
    "    'clf__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'clf__num_leaves': [15, 31, 63]\n",
    "}\n",
    "\n",
    "\n",
    "rs_lgb = RandomizedSearchCV(pipe_lgb\n",
    "                            , param_lgb\n",
    "                            , n_iter=8\n",
    "                            ,cv=tscv\n",
    "                            , scoring='roc_auc'\n",
    "                            ,n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "rs_lgb.fit(X_train, y_train)\n",
    "best_lgb = rs_lgb.best_estimator_\n",
    "\n",
    "metrics_lgb = evaluate_model(\"LightGBM\", best_lgb, X_test, y_test)\n",
    "print(\"LightGBM metrics:\", metrics_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a57616f-7c27-4605-b140-f5b0c7fcdc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AA - Adjusting the threshold to accomodate for ROC-AUC\n",
    "#sometimes lowering or raising this gives better results\n",
    "\n",
    "thresholds = [0.40, 0.45, 0.50]\n",
    "\n",
    "for t in thresholds:\n",
    "    #get the predicted probs of up\n",
    "    probs = best_lgb.predict_proba(X_test)[:, 1]\n",
    "    #convert based on threshold\n",
    "    preds_t = (probs > t).astype(int)\n",
    "\n",
    "    accur = accuracy_score(y_test, preds_t)\n",
    "    precision = precision_score(y_test, preds_t)\n",
    "    recall = recall_score(y_test, preds_t)\n",
    "    f1 = f1_score(y_test, preds_t)\n",
    "\n",
    "    print(f\"\\nThreshold {t}:\")\n",
    "    print(f\"  Accuracy  = {accur:.4f}\")\n",
    "    print(f\"  Precision  = {precision:.4f}\")\n",
    "    print(f\"  Recall  = {recall:.4f}\")\n",
    "    print(f\"  F1 Score  = {f1:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3d6c2-b1c7-4d4d-9c25-e4a2b5663aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AA - Adding ROC curves for all three models\n",
    "\n",
    "def roc_curve_plt(model, X_test, y_test, label):\n",
    "    #we first need the probability of UP for tomorrow\n",
    "\n",
    "    probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    #next we need the false positives and true positives\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "\n",
    "    #we compute AUC score based on the curve\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    #now we can do the actual plot\n",
    "    plt.plot(fpr, tpr, label = f\"{label} (AUC = {roc_auc:.4f})\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "\n",
    "# AA - we can plot ROC curve for each model\n",
    "roc_curve_plt(best_lr, X_test, y_test, \"Logistic Regression\")\n",
    "roc_curve_plt(best_rf, X_test, y_test, \"Random Forest\")\n",
    "roc_curve_plt(best_xgb, X_test, y_test, \"XGBoost\")\n",
    "roc_curve_plt(best_lgb, X_test, y_test, \"LightGBM\")\n",
    "\n",
    "# AA - we can add a diagonal line which indicates random guessing\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "\n",
    "plt.title(\"ROC Curves for all Models\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef304e4c-60a5-40df-a469-068062ca26ab",
   "metadata": {},
   "source": [
    "## Baseline Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaba905-7373-449c-b545-b2a5725f45bc",
   "metadata": {},
   "source": [
    "**Baseline Accuracy**\n",
    "\n",
    "The baseline (always predict UP) is 55.19%.\n",
    "Our best model (XGBoost at 55%) does not beat the baseline.\n",
    "\n",
    "This is important and aligns with market efficiency theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457194ea-72c7-4362-8594-62a19be2f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "baseline = y_test.value_counts(normalize=True).max()\n",
    "print(\"Baseline accuracy:\", baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77f152-7b5f-4d23-9ef2-475ca9c7dcb3",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5eb6a-7bdf-4207-a236-963a153606ea",
   "metadata": {},
   "source": [
    "**Feature Importance Insights**\n",
    "\n",
    "Most models rely heavily on:\n",
    "\n",
    "- Lagged returns\n",
    "- Volatility (rolling std)\n",
    "- MACD / MACD signal\n",
    "- SMA differences\n",
    "    \n",
    "These features reflect short-term momentum and market noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74133e4b-0c88-4c91-9a93-56b07266eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "# XGBoost\n",
    "try:\n",
    "    importances = pd.Series(best_xgb.named_steps['clf'].feature_importances_,\n",
    "                            index=X.columns)\n",
    "    importances.sort_values().plot(kind='bar', figsize=(10,4))\n",
    "    plt.title(\"Feature Importance (XGBoost)\")\n",
    "    plt.show()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Random Forest\n",
    "try:\n",
    "    rf_importances = pd.Series(best_rf.named_steps['clf'].feature_importances_,\n",
    "                               index=X.columns)\n",
    "    rf_importances.sort_values().plot(kind='bar', figsize=(10,4))\n",
    "    plt.title(\"Feature Importance (Random Forest)\")\n",
    "    plt.show()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Logistic Regression\n",
    "try:\n",
    "    coefs = pd.Series(best_lr.named_steps['clf'].coef_[0], index=X.columns)\n",
    "    coefs.sort_values().plot(kind='bar', figsize=(10,4))\n",
    "    plt.title(\"Feature Importance (Logistic Regression)\")\n",
    "    plt.show()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# LightGBM - AA\n",
    "\n",
    "try:\n",
    "    lgb_clf = best_lgb.named_steps['clf']\n",
    "    lgb_importances = pd.Series(lgb_clf.feature_importances_,index=X.columns)\n",
    "    lgb_importances.sort_values().plot(kind='bar', figsize=(10,4))\n",
    "    plt.title(\"Feature Importance (LightGBM)\")\n",
    "    plt.show() \n",
    "except Exception as e:\n",
    "    print(\"Could not plot Feature importance for LightGBM:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8a90d-244a-4ce1-80b6-f2706ae42b23",
   "metadata": {},
   "source": [
    "## Rolling 50-Day Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ccaa5-2af8-4f78-b7c7-c8e4921ef5ff",
   "metadata": {},
   "source": [
    "**Rolling Accuracy Interpretation**\n",
    "\n",
    "Rolling 50-day accuracy fluctuates near the baseline. Models perform well during trending markets but poorly during volatile or sideways periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36190be9-aa8d-4b71-adb7-7857f2aa6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "final_model = best_xgb\n",
    "\n",
    "# Align features to model training\n",
    "model_features = final_model.feature_names_in_\n",
    "X_test_aligned = X_test[model_features]\n",
    "\n",
    "preds = final_model.predict(X_test_aligned)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Date\": data['Date'].iloc[split_index:],\n",
    "    \"Actual\": y_test.values,\n",
    "    \"Pred\": preds})\n",
    "\n",
    "results['Correct'] = results['Actual'] == results['Pred']\n",
    "\n",
    "results.set_index(\"Date\")['Correct'].rolling(50).mean().plot(figsize=(12,5))\n",
    "plt.title(\"Rolling 50-Day Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca96032-d259-4d5d-b476-6bb559c0631d",
   "metadata": {},
   "source": [
    "### Pick and save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe7b47-e836-4806-820a-a8b3ccab2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "\n",
    "#AA - adding to this by model selection using ROC-AUC\n",
    "# candidates = {\n",
    "#     \"logreg\": (best_lr, acc_lr),\n",
    "#     \"rf\": (best_rf, acc_rf),\n",
    "#     \"xgb\": (best_xgb, acc_xgb)}\n",
    "\n",
    "#AA\n",
    "candidates = {\n",
    "    \"Logistic Regression\": metrics_lr,\n",
    "    \"Random Forest\": metrics_rf,\n",
    "    \"XGBoost\": metrics_xgb,\n",
    "    \"LightGBM\": metrics_lgb}\n",
    "\n",
    "# best_name = max(candidates, key=lambda k: candidates[k][1])\n",
    "# best_model = candidates[best_name][0]\n",
    "\n",
    "\n",
    "best_name = max(candidates, key=lambda name: candidates[name][\"roc_auc\"]) #AA\n",
    "best_model_metrics = candidates[best_name] #AA\n",
    "\n",
    "print(f\"Best model(based on ROC-AUC): {best_name}\") #RS/AA updated to make sure its know to be based off ROC-AUC\n",
    "print(f\"Metrics: {best_model_metrics}\") #AA\n",
    "\n",
    "#AA - Adding to save the actual model itself\n",
    "\n",
    "if best_name == \"Logistic Regression\":\n",
    "    final_model = best_lr\n",
    "elif best_name == \"Random Forest\":\n",
    "    final_model = best_rf\n",
    "elif best_name == \"LightGBM\":\n",
    "    final_model = best_lgb\n",
    "else:\n",
    "    final_model = best_xgb\n",
    "    \n",
    "joblib.dump(final_model, \"sp500_model_capstone.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96920f-d798-44b8-a8ab-435f2b583f65",
   "metadata": {},
   "source": [
    "To choose the best model, we compared ROC-AUC scores of all four of our models:\n",
    "\n",
    "Logistic Regression\n",
    "Random Forest\n",
    "XGBoost\n",
    "LightGBM\n",
    "We choose ROC-AUC because its better for data with class imbalance. Accuracy can be especially be misleading when there is class imbalance.\n",
    "\n",
    "The model with the highest ROC-AUC score is then selected as our final model. This model is then used for the daily predictions and threshold tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374bb2e-798c-4b62-b90e-212a4ec0c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AA - Creating a quick data frame of all model metrics\n",
    "\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Random Forest\", \"XGBoost\", \"LightGBM\"],\n",
    "    \"Accuracy\": [metrics_lr[\"accuracy\"]\n",
    "                 ,metrics_rf[\"accuracy\"]\n",
    "                 ,metrics_xgb[\"accuracy\"]\n",
    "                 ,metrics_lgb[\"accuracy\"]],\n",
    "    \"ROC_AUC\": [metrics_lr[\"roc_auc\"]\n",
    "                ,metrics_rf[\"roc_auc\"]\n",
    "                ,metrics_xgb[\"roc_auc\"]\n",
    "                ,metrics_lgb[\"roc_auc\"]]\n",
    "})\n",
    "\n",
    "print(\"Overall Model performace summary for the test set:\")\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf75e1e-be5b-4a47-b639-bd7c4cb62e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AA - incorporate the best threshold\n",
    "\n",
    "threshold_final = 0.40\n",
    "\n",
    "def final_pred(model, X, threshold = threshold_final):\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    return (probs > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced38621-f07a-4434-898e-e8ecaf841e9a",
   "metadata": {},
   "source": [
    "## DAILY PREDICTION PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ce8ba-2bd4-492b-8e49-363d40a36b55",
   "metadata": {},
   "source": [
    "**Daily Prediction Interpretation**\n",
    "\n",
    "The random forest model, chosen by ROC-AUC predicts the next day’s direction and probability for UP.\n",
    "\n",
    "- This model used the latest technical features and give the probability of the next day being UP.\n",
    "- Instead of using the default threshold of 0.50, we chose a 0.40 threshold because it gave better accuracy and balance for our tests\n",
    "- This threshold basically says if the probability of UP is above 40% we predict UP otherwise the prediction is DOWN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52077190-d61e-4621-96ab-76f611af40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS\n",
    "model = final_model #AA - UPDATED\n",
    "\n",
    "# Align feature order\n",
    "model_features = model.feature_names_in_\n",
    "X_latest = X.iloc[[-1]][model_features]\n",
    "\n",
    "pred = final_pred(model, X_latest)[0]\n",
    "prob = model.predict_proba(X_latest)[0][1]\n",
    "\n",
    "# Convert to percent and round to 2 decimals\n",
    "prob_percent = round(prob * 100, 2)\n",
    "\n",
    "print(\"Prediction for next trading day after:\", data['Date'].iloc[-1])\n",
    "print(\"Predicted direction:\", \"UP\" if pred == 1 else \"DOWN\")\n",
    "print(f\"Probability of UP: {prob_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22604ef4-796a-4e60-a69a-61c8d08c4d79",
   "metadata": {},
   "source": [
    "### Final Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96999c25-18e0-4279-b1c3-26a35cca3962",
   "metadata": {},
   "source": [
    "This project explored predicting daily S&P 500 direction using traditional technical indicators.\n",
    "    \n",
    "**Key Findings**\n",
    "    \n",
    "No model beat the baseline accuracy (predict UP every day).\n",
    "\n",
    "**Comparison of baseline to models**\n",
    "\n",
    "- We have a simple baseline rule of always predicting UP which gets an accuracy of around 55% on the test set.\n",
    "- All tuned models do not beat the baseline and had around 43-47% accuracy\n",
    "\n",
    "**ROC-AUC**\n",
    "\n",
    "- The ROC-AUC for all models reched around 0.47-0.48 on the test set.\n",
    "- Rodom Forest was our best model at about 0.48, however this isn't better than random guessing would be at 0.50.\n",
    "\n",
    "**Improvement Attempts**\n",
    "\n",
    "- Addition of more features such as Volatility, Bollinger Bands, Momentum, OBV.\n",
    "- Handling of class imbalance and a tuned threshold\n",
    "\n",
    "While these improvements give us a more robust evaluation but didn't create a strong predictive model.\n",
    "\n",
    "**What this means/Bigger Picture**\n",
    "\n",
    "- Down days are still very hard to predict over up days.\n",
    "- the models stuggled to reliably separate a down versus up days, supporting the idea that short-term price direction is hard to forecast.\n",
    "  \n",
    "**Interpretation**\n",
    "\n",
    "The results show that daily market movement is hard to predict, even with additional features and machine learning models. None of them were able to beat baseline and this strongly suggests that daily changes are mostly noise, making the next day predictions difficult.\n",
    "\n",
    "**Possible Next Steps**\n",
    "\n",
    "- Include macroeconomic indicators (VIX, yield curve, unemployment).\n",
    "- Explore longer-horizon predictions (weekly/monthly).\n",
    "- Add regime detection (bull/bear markets).\n",
    "- Try hybrid models or ensemble approaches.\n",
    "  \n",
    "The notebook provides a realistic, interpretable analysis consistent with both data science and financial theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a6e6a-9a7e-4882-89c4-2ebc1f32dd98",
   "metadata": {},
   "source": [
    "**Review Notes:** My only feedback is I feel like some efforts were duplicated and we could've made it a bit cleaner. For example the split logic was done like this split_index = int(0.8 * len(X))....\n",
    "\n",
    "and then later done like this TimeSeriesSplit(n_splits=5) - AA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea79e1f-9394-4dee-8100-0af0fdf83114",
   "metadata": {},
   "source": [
    "This notebook was run end-to-end on my local machine without errors - AA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edcf36-64eb-4a8c-baaa-6ab9e4ca15fb",
   "metadata": {},
   "source": [
    "This notebook was run end-to-end on my local machine without errors - RS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DSE6211)",
   "language": "python",
   "name": "dse6211"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
